{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyObrvh6X37pU5sz9wyubq+G","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gated recurrent unit (GRU)\n\nGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.\n\nThe GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM\n\nSome documentation\n- [Wikipedia - GRUs](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n- [Pytorch - RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n- [Pytorch - GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n\n\n# Imports","metadata":{"id":"IDQ0ZQJoeq6v"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\n# Device \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"2rnQ7hvOeNhh","execution":{"iopub.status.busy":"2024-08-08T17:08:42.347875Z","iopub.execute_input":"2024-08-08T17:08:42.348291Z","iopub.status.idle":"2024-08-08T17:08:47.547449Z","shell.execute_reply.started":"2024-08-08T17:08:42.348260Z","shell.execute_reply":"2024-08-08T17:08:47.546152Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Model\nIn this application the RNN we can view the images from the MNIST dataset (28x28) as 28 time sequences with 28 features. Of course, normally we wouldn use RNN with images, there are way better architectures for this image related tasks.","metadata":{"id":"tVIhuFdHevrr"}},{"cell_type":"code","source":"# Hyperparameter\ninput_size = 28\nsequence_length = 28\nnum_layers = 2\nhidden_size = 256\nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n    \n# Create the model\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n        \n    def forward(self, x):\n        # initialization of hidden states\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # (num_layers, N_mini_batches, hidden_size)\n        # forward prop\n        out, _ = self.gru(x, h0) # here we dont store the hidden state, because every example has its own hidden state\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        return out","metadata":{"id":"De4IB4j9ezAX","outputId":"f5ec245b-3d21-47dd-d37a-0859b37e2273","execution":{"iopub.status.busy":"2024-08-08T17:08:47.549529Z","iopub.execute_input":"2024-08-08T17:08:47.550024Z","iopub.status.idle":"2024-08-08T17:08:47.560751Z","shell.execute_reply.started":"2024-08-08T17:08:47.549991Z","shell.execute_reply":"2024-08-08T17:08:47.559486Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data\n- MNIST: 28x28 pixels\n- When we load the dataset, the shape will be (batch_size, 1, 28, 28)","metadata":{"id":"NivC79aefIGV"}},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"id":"27vHygWwfKiU","outputId":"5cc9f5ad-68e5-484f-dc39-93eb6ea13208","execution":{"iopub.status.busy":"2024-08-08T17:13:45.244905Z","iopub.execute_input":"2024-08-08T17:13:45.245289Z","iopub.status.idle":"2024-08-08T17:13:45.351712Z","shell.execute_reply.started":"2024-08-08T17:13:45.245261Z","shell.execute_reply":"2024-08-08T17:13:45.350196Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"PohFFebhe2K7"}},{"cell_type":"code","source":"# Intialize NN\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the NN\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Get data to device\n        data = data.to(device=device).squeeze(1) #remove the dimention 1 in (Nx1x28x28)\n        targets = targets.to(device=device)\n\n        # Forward propagation\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # Backward propagation\n        optimizer.zero_grad() # initialize all gradients to zero for each batch\n        loss.backward()\n\n        # Gradient descent or Adam step\n        optimizer.step()","metadata":{"id":"Xucyzc8Ge8RH","execution":{"iopub.status.busy":"2024-08-08T17:08:55.848389Z","iopub.execute_input":"2024-08-08T17:08:55.848866Z","iopub.status.idle":"2024-08-08T17:12:31.218610Z","shell.execute_reply.started":"2024-08-08T17:08:55.848826Z","shell.execute_reply":"2024-08-08T17:12:31.217513Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Performance","metadata":{"id":"UvS7Go0de-yH"}},{"cell_type":"code","source":"# Check accuracy on training and test sets\ndef check_accuracy(loader, model):\n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on test data\")\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)  # scores is 64x10 and we want to know which one of those the is the maximum value, so in max: dim=1\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(f'got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n    model.train()","metadata":{"id":"w9ZXp1PCfADg","execution":{"iopub.status.busy":"2024-08-08T17:12:31.220589Z","iopub.execute_input":"2024-08-08T17:12:31.221051Z","iopub.status.idle":"2024-08-08T17:12:31.228612Z","shell.execute_reply.started":"2024-08-08T17:12:31.221010Z","shell.execute_reply":"2024-08-08T17:12:31.227467Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"check_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","metadata":{"id":"hLDIDqEPrWhk","outputId":"94a9d564-477f-43db-88ca-477c4af08078","execution":{"iopub.status.busy":"2024-08-08T17:12:31.229969Z","iopub.execute_input":"2024-08-08T17:12:31.230333Z","iopub.status.idle":"2024-08-08T17:13:13.914187Z","shell.execute_reply.started":"2024-08-08T17:12:31.230289Z","shell.execute_reply":"2024-08-08T17:13:13.913093Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Checking accuracy on training data\ngot 59305 / 60000 with accuracy 98.84\nChecking accuracy on test data\ngot 9863 / 10000 with accuracy 98.63\n","output_type":"stream"}]}]}