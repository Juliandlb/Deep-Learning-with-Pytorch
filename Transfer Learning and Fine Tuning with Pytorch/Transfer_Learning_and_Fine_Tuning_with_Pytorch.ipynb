{"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOnIP8xmn7sS2CVpet4uP60","gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":32.287315,"end_time":"2024-08-07T23:34:29.153002","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-07T23:33:56.865687","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer Learning and Fine Tuning with Pytorch\n\nPytorch comes with the model torchvision.models that provides a collection of pre-trained and pre-defined neural network architectures for computer vision tasks. These models can be used for tasks such as image classification, object detection, segmentation, and more.\n\nThen, we will use transfer learning for the current dataset (CIFAR10). Here we will freze some layers, to not do backprop on then, and only do it for the last layers.\n\nSome documentation\n- [Pytorch - torchvision.models](https://pytorch.org/vision/0.9/models.html)\n- [CIFAR - 10](https://www.cs.toronto.edu/~kriz/cifar.html)","metadata":{"id":"IDQ0ZQJoeq6v","papermill":{"duration":0.003911,"end_time":"2024-08-07T23:33:59.852728","exception":false,"start_time":"2024-08-07T23:33:59.848817","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Imports\nimport torch\nimport torchvision\nimport torch.nn as nn  \nimport torch.optim as optim \nimport torch.nn.functional as F \nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets \nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:26:03.813637Z","iopub.execute_input":"2024-08-13T22:26:03.814012Z","iopub.status.idle":"2024-08-13T22:26:03.820022Z","shell.execute_reply.started":"2024-08-13T22:26:03.813981Z","shell.execute_reply":"2024-08-13T22:26:03.818944Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nnum_classes = 10\nlearning_rate = 1e-3\nbatch_size = 1024\nnum_epochs = 5\n\n# Load pretrain model & modify it\nmodel = torchvision.models.vgg16(weights=\"DEFAULT\")\n\n# Finetuning needs requires_grad = False to not change this parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.avgpool = nn.Identity()\nmodel.classifier = nn.Sequential(\n    nn.Linear(512, 100), \n    nn.ReLU(), \n    nn.Linear(100, num_classes)\n)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:26:03.822961Z","iopub.execute_input":"2024-08-13T22:26:03.823658Z","iopub.status.idle":"2024-08-13T22:26:05.600569Z","shell.execute_reply.started":"2024-08-13T22:26:03.823611Z","shell.execute_reply":"2024-08-13T22:26:05.599604Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): Identity()\n  (classifier): Sequential(\n    (0): Linear(in_features=512, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=10, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Data and Training","metadata":{}},{"cell_type":"code","source":"# Load Data\ntrain_dataset = datasets.CIFAR10(\n    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\nprint(f'Training dataset size: {len(train_dataset)}')\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train Network\nfor epoch in range(num_epochs):\n    losses = []\n\n    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n        # Get data to cuda if possible\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        losses.append(loss.item())\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:26:05.602221Z","iopub.execute_input":"2024-08-13T22:26:05.602511Z","iopub.status.idle":"2024-08-13T22:27:06.859713Z","shell.execute_reply.started":"2024-08-13T22:26:05.602486Z","shell.execute_reply":"2024-08-13T22:27:06.858852Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nTraining dataset size: 50000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49/49 [00:12<00:00,  4.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost at epoch 0 is 1.59220\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49/49 [00:12<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost at epoch 1 is 1.21906\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49/49 [00:11<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost at epoch 2 is 1.14724\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49/49 [00:12<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost at epoch 3 is 1.11182\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 49/49 [00:12<00:00,  4.07it/s]","output_type":"stream"},{"name":"stdout","text":"Cost at epoch 4 is 1.08799\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Accuracy check","metadata":{}},{"cell_type":"code","source":"# Check accuracy on training & test to see how good our model\ndef check_accuracy(loader, model):\n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on test data\")\n\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\n            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n        )\n\n    model.train()\n\n\ncheck_accuracy(train_loader, model)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:27:06.861022Z","iopub.execute_input":"2024-08-13T22:27:06.861320Z","iopub.status.idle":"2024-08-13T22:27:13.940796Z","shell.execute_reply.started":"2024-08-13T22:27:06.861294Z","shell.execute_reply":"2024-08-13T22:27:13.939868Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Checking accuracy on training data\nGot 31328 / 50000 with accuracy 62.66\n","output_type":"stream"}]}]}